{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://mistral.ai/images/logo_hubc88c4ece131b91c7cb753f40e9e1cc5_2589_256x0_resize_q97_h2_lanczos_3.webp\" width=\"100\"/>\n",
    "<img src=\"https://cdn.iconscout.com/icon/free/png-256/free-python-3521655-2945099.png?f=webp&w=256\" width=\"35\"/>\n",
    "</p>\n",
    "\n",
    "<h1 align=\"center\">Génération d'un Dataset  pour proceder au fine-tuning d'un modèle Mistral.ai</h1>\n",
    "\n",
    "---\n",
    "<p style='text-align: justify;'>\n",
    "Ce notebook est un outil exploratoire de génération d'un dataset pour proceder au fine-tuning d'un modèle Mistral.ai. Il permet de générer un grand nombre de dialogue sur base d'un prompt. Ce prompt est construit sur base d'une question et d'une réponse. Le prompt est ensuite utilisé pour générer un grand nombre de dialogue. Ces dialogues sont ensuite stockés dans un fichier json. Ce fichier json est ensuite utilisé pour proceder au fine-tuning d'un modèle Mistral.ai.\n",
    "\n",
    "Dans cet exemple, nous allons générer un dataset pour un CV. La question est \"Comment je m'appelle\" et la réponse est \"Tu t'appelles [nom] [prénom]\". Le prompt est ensuite utilisé pour générer 50 variations de la question et de la réponse. Le but est de montrer la capacité du modèle à générer un dataset de qualité de dialogue sur base d'un prompt.\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## Installation et importation des données du CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Ce morceaux de code permet de charger et de pouvoir naviguer dans les données disponibles dans le json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org Unidecode mistralai os json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quel est l'adresse de Jean-Baptiste ASTRUZ\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Chemin vers le fichier JSON\n",
    "chemin_fichier = 'data.json'\n",
    "\n",
    "# Ouvrir et lire le fichier JSON\n",
    "with open(chemin_fichier, 'r') as fichier:\n",
    "    CV_data = json.load(fichier)\n",
    "\n",
    "question = [\"Comment je m'appelle\", \"Quand suis je né\", \"Quel est ma nationnalité\", f\"\"\"Quel est l'adresse de {CV_data[0][\"Identite\"][\"Prenom\"] + ' ' + CV_data[0][\"Identite\"][\"Nom\"]}\"\"\", \n",
    "            f\"\"\"Quel est le numéro de téléphone de {CV_data[0][\"Identite\"][\"Prenom\"] + ' ' + CV_data[0][\"Identite\"][\"Nom\"]}\"\"\", f\"\"\"Quel est l'adresse e-mail de {CV_data[0][\"Identite\"][\"Prenom\"] + ' ' + CV_data[0][\"Identite\"][\"Nom\"]}\"\"\", \n",
    "            f\"\"\"Quel est le lien du profil linkedIn de {CV_data[0][\"Identite\"][\"Prenom\"] + ' ' + CV_data[0][\"Identite\"][\"Nom\"]}\"\"\", \"Quel est ma dernière expérience professionnelle\", \"Combien de poste ai-je occupé par le passé\", \"Quel langue je parle\",\n",
    "            \"Combien de langue je parle\", \"Quel est mon niveau d'anglais\", \"quel est mon niveau de Français\"]\n",
    "reponse = [f\"\"\"Tu t\\'appelles {CV_data[0][\"Identite\"][\"Prenom\"] + ' ' + CV_data[0][\"Identite\"][\"Nom\"]}\"\"\", f\"\"\"tu es né le {CV_data[0][\"Identite\"][\"Date de naissance\"]}\"\"\", \n",
    "           f\"\"\"Tu es de nationalité {CV_data[0][\"Identite\"][\"Nationalite\"]}\"\"\", f\"\"\"Tu habites au {CV_data[0][\"Identite\"][\"Adresse\"]}\"\"\", f\"\"\"Tu peux me joindre au {CV_data[0][\"Identite\"][\"Numero de telephone\"]}\"\"\",\n",
    "           f\"\"\"Tu peux me contacter par e-mail à l\\'adresse {CV_data[0][\"Identite\"][\"Adresse e-mail\"]}\"\"\", f\"\"\"Tu peux consulter mon profil LinkedIn à l\\'adresse suivante {CV_data[0][\"Identite\"][\"LinkedIn\"]}\"\"\",\n",
    "           f\"\"\"Ma dernière expérience professionnelle a été en tant que {list(CV_data[0][\"Experiences Professionelles\"])[0]} chez {CV_data[0][\"Experiences Professionelles\"][list(CV_data[0][\"Experiences Professionelles\"])[0]][\"company\"]}\"\"\",\n",
    "           f\"\"\"J\\'ai occupé un total de {len(CV_data[0][\"Experiences Professionelles\"])} postes au cours de ma carrière.\"\"\", f\"\"\"Je parle {len(CV_data[0][\"LANGUES\"])} langues, dont {\", \".join(CV_data[0][\"LANGUES\"])}\"\"\", f\"\"\"je parle {len(CV_data[0][\"LANGUES\"])} langues\"\"\",\n",
    "           f\"\"\"Mon niveau d\\'anglais est {CV_data[0][\"LANGUES\"][\"English\"][\"level\"]}\"\"\", f\"\"\"Mon niveau de Français est {CV_data[0][\"LANGUES\"][\"French\"][\"level\"]}\"\"\"]\n",
    "\n",
    "question[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Création d'un prompt permettant de demander la génération d'un nombre arbitrairement grand de dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Ce prompt à pour but de générer un ordre dynamique qui puisse permettre de générer un grand nombre de réponse sur base d'un grand nombre de questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [None] * len(question)\n",
    "for i in range(len(question)):\n",
    "    Q = question[i]\n",
    "    R = reponse[i]\n",
    "    nb_variation = 200\n",
    "    prompt[i] = f\"\"\"Je souhaite que tu me génères exactement {nb_variation} variations de la question {Q}?. utilise des mot comme \"moi\", \"mon\". De plus, je souhaite que pour chaque variations, tu puisses donner la paraphrase de la réponse qui est \"{R}.\". N'hesites pas à reformuler la réponse\"\"\" + \"\"\"\n",
    "    Génère la réponse sous forme de JSON uniquement avec la structure suivante:\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : \"Variation 1 de la question\"\n",
    "            },\n",
    "            {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : \"Variation 1 de la réponse\"\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : \"Variation 2 de la question\"\n",
    "            },\n",
    "            {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : \"Variation 2 de la réponse\"\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : \"Variation 3 de la question\"\n",
    "            },\n",
    "            {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : \"Variation 3 de la réponse\"\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            ...\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\" : \"Variation {nb_variation} de la question\"\n",
    "            },\n",
    "            {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : \"Variation {nb_variation} de la réponse\"\n",
    "            }\n",
    "        ]\n",
    "    }\"\"\"+f\"\"\"\n",
    "    Ne rajoute pas de texte en dehors de la structure JSON. Il ne faut pas quil y ait le paramètre \"added flair\" dans la structure du JSON. ni avant ni après. Ne met pas de mise en forme markdown. De plus, tu génères EXACTEMENT {nb_variation} variations. Tu ne parles qu'en français. je génère pas plus de {nb_variation} variations. Ne rajoute pas de texte en dehors de la structure JSON. ni avant ni après. Ne met pas de mise en forme markdown. De plus, tu génères EXACTEMENT {nb_variation} variations. Tu ne parles qu'en français. je génère pas plus de {nb_variation} variations.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Installation et importation des packages nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "On récupère les bons packages pour pouvoir faire le travail d'IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient # Import the MistralClient from the mistralai.client module\n",
    "from mistralai.models.chat_completion import ChatMessage # Import the ChatMessage from the mistralai.models.chat_completion module\n",
    "from dotenv import load_dotenv # Import the load_dotenv function from the dotenv module\n",
    "import os # Import the os module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Définition du client Mistral.ai et du modèle à utiliser\n",
    "On défini le client et le modèle d'IA que l'on va utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "model = \"open-mixtral-8x7b\"\n",
    "api_key= os.getenv('MISTRAL_API_KEY')\n",
    "client = MistralClient(api_key=api_key, timeout=120)\n",
    "responses = []\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "On execute le prompt pour pouvoir récupérer les exemples de diaolgue fournis en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, Pr in enumerate(prompt) :\n",
    "    messages.append(ChatMessage(role=\"user\", content=Pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(messages)) :\n",
    "    responses.append(client.chat(model=model, messages= [messages[i]], response_format={\"type\": \"json_object\"}, temperature=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Enregistrement des résultats\n",
    "On va simplement enregistrer les résultats pour pouvoir les charger plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Comment_je_m'appelle.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quand_suis_je_né.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_est_ma_nationnalité.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_est_l'adresse_de_Jean-Baptiste_ASTRUZ.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_est_le_numéro_de_téléphone_de_Jean-Baptiste_ASTRUZ.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_est_l'adresse_e-mail_de_Jean-Baptiste_ASTRUZ.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_est_le_lien_du_profil_linkedIn_de_Jean-Baptiste_ASTRUZ.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_est_ma_dernière_expérience_professionnelle.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Combien_de_poste_ai-je_occupé_par_le_passé.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_langue_je_parle.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Combien_de_langue_je_parle.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Quel_est_mon_niveau_d'anglais.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/quel_est_mon_niveau_de_Français.json.\n",
      "Le fichier JSON a été sauvegardé avec succès dans le fichier json/Total.json.\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "concat_json_object = list()\n",
    "for i in range(len(messages)):\n",
    "    \n",
    "    json_text = unidecode(responses[i].choices[0].message.content).strip().split('],')\n",
    "    modified_json_strings = [\"{\" + json_str + \"]}\" for json_str in json_text[1:len(json_text)-1]]\n",
    "    modified_json_strings[0] = json_text[0] + \"]}\"\n",
    "    modified_json_strings[-1] = \"{\" + json_text[-1]\n",
    "\n",
    "    json_object = [json.loads(json_str) for json_str in modified_json_strings]\n",
    "\n",
    "    # Enregistrer les données JSON dans un fichier\n",
    "    with open(f'json/{question[i].replace(\" \", \"_\")}.json', 'w') as json_file:\n",
    "        json.dump(json_object, json_file, indent=4)\n",
    "    \n",
    "    for item in json_object: concat_json_object.insert(0, item)\n",
    "\n",
    "    print(f\"Le fichier JSON a été sauvegardé avec succès dans le fichier json/{question[i].replace(\" \", \"_\")}.json.\")\n",
    "\n",
    "with open('json/Total.json', 'w') as json_file:\n",
    "    json.dump(concat_json_object, json_file, indent=4)\n",
    "\n",
    "print(f\"Le fichier JSON a été sauvegardé avec succès dans le fichier json/Total.json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de pouvoir ire utiliser les données, il faut les convertir en format JSONL, pour ce faire, il suffit se supprimer les retours à la ligne.<br/>\n",
    "Le code qui suit permet de le faire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier JSONL a été créé avec succès.\n"
     ]
    }
   ],
   "source": [
    "with open('json/Total.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open('Total.jsonl', 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print(\"Le fichier JSONL a été créé avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification du nombre de messages générés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le rôle 'user' apparaît 12 fois.\n",
      "Le rôle 'assistant' apparaît 12 fois.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Charger les données JSON depuis le fichier\n",
    "with open('json/Total.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker le nombre d'occurrences de chaque rôle\n",
    "role_count = {}\n",
    "\n",
    "for i in range(12):\n",
    "    # Parcourir la liste de messages\n",
    "    for message in data[i]['messages']:\n",
    "        role = message['role']\n",
    "        # Si le rôle est déjà dans le dictionnaire, incrémenter le compteur\n",
    "        if role in role_count:\n",
    "            role_count[role] += 1\n",
    "        # Sinon, ajouter le rôle au dictionnaire avec un compteur initial de 1\n",
    "        else:\n",
    "            role_count[role] = 1\n",
    "\n",
    "# Afficher le nombre d'occurrences de chaque rôle\n",
    "for role, count in role_count.items():\n",
    "    print(f\"Le rôle '{role}' apparaît {count} fois.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons utilisé le modèle Mistral.ai pour générer 50 variations d'une question donnée et leurs réponses correspondantes. Nous avons enregistré les résultats dans un fichier JSON pour une utilisation ultérieure.\n",
    "\n",
    "Le résultat est satisfaisant, et permet de générer un grand nombre de variations d'une question et de ses réponses. Cela peut être utile pour l'entraînement de modèles de dialogue ou pour la génération de données de test. Les prochaines étapes consiste à permettre de générer des variations de plusieurs questions en même temps, et de pouvoir les générer en parallèle pour gagner du temps. il faudra aussi pouvoir générer des variations de questions et de réponses en plusieurs langues et centraliser les résultats dans un même fichier JSON.\n",
    "\n",
    "Ce notebook peut être utilisé comme point de départ pour explorer davantage les capacités du modèle Mistral.ai et pour générer des données de dialogue de haute qualité pour diverses applications. il faut aussi implementer un systeme pour entrainer l'IA sur base de ces données générées.\n",
    "\n",
    "<u>nb</u>: Il est compliquer de forcer le nombre de 50 variations. Il est possible que le nombre de variations générées soit inférieur à 50. Il faudra donc faire attention à ce point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancement du fine tuning\n",
    "\n",
    "NB : le programme ne reconnait pas les charactère accentués, il faut donc les remplacer par des charactères non accentués. ça peut être fait avec la fonction unidecode.unidecode() (A VERIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier JSONL a été créé avec succès.\n"
     ]
    }
   ],
   "source": [
    "#temp\n",
    "with open('json/Total.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open('Total.jsonl', 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print(\"Le fichier JSONL a été créé avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org mistralai --upgrade\n",
    "import os\n",
    "import json\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "with open(\"Total.jsonl\", \"rb\") as f:\n",
    "    training_data = client.files.create(file=(\"Total.jsonl\", f))\n",
    "\n",
    "#with open(\"Total_Validation.jsonl\", \"rb\") as f:\n",
    "#    validation_data = client.files.create(file=(\"Total_Validation.jsonl\", f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='522cb9e9-70b4-49be-a8fa-561d130961eb' hyperparameters=TrainingParameters(training_steps=10, learning_rate=0.0001) fine_tuned_model=None model='open-mistral-7b' status='QUEUED' job_type='FT' created_at=1723105056 modified_at=1723105056 training_files=['ff649c12-487b-46c8-91d8-9946058163ab'] validation_files=[] object='job' integrations=[]\n"
     ]
    }
   ],
   "source": [
    "from mistralai.models.jobs import TrainingParameters\n",
    "\n",
    "created_jobs = client.jobs.create(\n",
    "    model=\"open-mistral-7b\",\n",
    "    training_files=[training_data.id],\n",
    "    hyperparameters=TrainingParameters(\n",
    "        training_steps=10,\n",
    "        learning_rate=0.0001,\n",
    "        )\n",
    ")\n",
    "print(created_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Event(name='status-updated', data={'status': 'SUCCESS'}, created_at=1723105153),\n",
       " Event(name='status-updated', data={'status': 'RUNNING'}, created_at=1723105058),\n",
       " Event(name='status-updated', data={'status': 'QUEUED'}, created_at=1723105057),\n",
       " Event(name='status-updated', data={'status': 'VALIDATED'}, created_at=1723105057),\n",
       " Event(name='status-updated', data={'status': 'RUNNING'}, created_at=1723105057),\n",
       " Event(name='status-updated', data={'status': 'QUEUED'}, created_at=1723105056)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_jobs = client.jobs.retrieve(created_jobs.id)\n",
    "retrieved_jobs.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:open-mistral-7b:5a962433:20240808:522cb9e9\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_jobs.fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai mis trop de dialogue dans chaque conversation, il faut que je découpe les besoin par en conversation de type question/réponse. le mieux est de tester de modifier le prompt pour pouvoir changer le résultat de la génération."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
